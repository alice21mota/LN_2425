{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ai0bSdHEiTx"
      },
      "source": [
        "# **Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ8KKxo8FAJH"
      },
      "source": [
        "# 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTjSM6HyENl0",
        "outputId": "29e56c21-c05e-4997-cc55-245b3958ab37"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.6' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoMqPiSKFLap"
      },
      "source": [
        "# 2. Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IslE7J7IE97K",
        "outputId": "314d3d49-1abf-4fed-da02-2e04597f6c33"
      },
      "outputs": [],
      "source": [
        "# Add the dataset the sample_data folder (on the left side)\n",
        "\n",
        "# Load the dataset\n",
        "train = pd.read_csv('/content/sample_data/train.txt', sep='\\t', names=['title', 'from', 'genre', 'director', 'plot'])\n",
        "\n",
        "x_train = train['genre']\n",
        "y_train = train['plot']\n",
        "#x_test = test['title', 'form', 'director', 'plot']\n",
        "\n",
        "\n",
        "# Inspect the first few rows\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PROoG8OkJpRA"
      },
      "source": [
        "# 2. Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hfEEj6MJuZL",
        "outputId": "1466d063-c1a8-4c06-ec2f-53d9604ba12b"
      },
      "outputs": [],
      "source": [
        "# Pre-processing elements\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "including = ['no', 'nor', 'not', 'but', 'against', 'only']\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# def processSentence(s):\n",
        "#     words=re.split(\"\\\\s+\",s)\n",
        "#     stemmed_words=[porter_stemmer.stem(word=w) for w in words]\n",
        "#     return ' '.join(stemmed_words)\n",
        "\n",
        "def apply_preprocessing(text):\n",
        "\n",
        "  lowered = text.lower()\n",
        "  lowered_re = re.sub(r'\\(.*?\\)|[^a-zA-Z\\s]', '', lowered)\n",
        "  tokens = word_tokenize(lowered_re, \"english\")\n",
        "\n",
        "  # Remove ponctuation\n",
        "  for token in tokens:\n",
        "        if(all(char in string.punctuation for char in token)):\n",
        "            tokens.remove(token)\n",
        "\n",
        "  filtered_tokens = [word for word in tokens if word not in stop]\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  processed_tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in filtered_tokens]\n",
        "\n",
        "  return ' '.join(processed_tokens)\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "y_train = y_train.apply(apply_preprocessing)\n",
        "\n",
        "\n",
        "print(y_train[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
