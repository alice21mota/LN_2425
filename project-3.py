# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DsbvBKi4zZbkOYNZUFLxh8RDO-v6tbaj

# **Project**

# 1. Imports
"""

import numpy as np
import pandas as pd
import nltk
import re
import string
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import StratifiedKFold
from sklearn import svm
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.metrics import classification_report, accuracy_score

nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('punkt')
nltk.download('wordnet')

"""# 2. Dataset load"""

# Add the dataset the sample_data folder (on the left side)

# Load the dataset
train = pd.read_csv('/content/sample_data/train.txt', sep='\t', names=['title', 'from', 'genre', 'director', 'plot'])

X = train['plot']
y = train['genre']
#x_test = test['title', 'form', 'director', 'plot']


# Inspect the first few rows
print(train.head())

"""# 3. Pre-processing"""

# Pre-processing elements

stop = stopwords.words('english')
including = ['no', 'nor', 'not', 'but', 'against', 'only']
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

def apply_preprocessing(text):

  lowered = text.lower()
  # Remove everything that is not a word or inside parenthesis
  lowered_re = re.sub(r'\(.*?\)|[^a-zA-Z\s]', '', lowered)
  tokens = word_tokenize(lowered_re, "english")

  # Remove ponctuation
  for token in tokens:
        if(all(char in string.punctuation for char in token)):
            tokens.remove(token)

  filtered_tokens = [word for word in tokens if word not in stop]

  lemmatizer = WordNetLemmatizer()
  processed_tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in filtered_tokens]

  return ' '.join(processed_tokens)


# Apply preprocessing
X = X.apply(apply_preprocessing)

# spliting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.125, random_state=1)



print(X.head())

"""# 4. Apply Naive Bayes"""

# Create a pipeline with custom tokenizer
nb_pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', MultinomialNB()),
])

# Parameters for grid search
parameters = {
    'vect__max_features': [1000, 3000, 5000, 10000, 15000, 20000],
    'tfidf__use_idf': [True, False],
    'clf__alpha': [0.1, 0.2, 0.5, 1.0],
}

# Stratified K-Fold cross-validation
skf = StratifiedKFold(n_splits=3)

# Perform Grid Search
grid_search = GridSearchCV(nb_pipeline, parameters, cv=skf)
grid_search.fit(X_train, y_train)

"""# 5. Evaluation

"""

# Get the best classifier and make predictions
best_classifier = grid_search.best_estimator_
y_test_pred = best_classifier.predict(X_test)

# Display best parameters
print(grid_search.best_params_)

# Classification report and accuracy score
print("Classification Report:")
print(classification_report(y_test, y_test_pred))
print("Accuracy Score:", accuracy_score(y_test, y_test_pred))