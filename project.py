# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DsbvBKi4zZbkOYNZUFLxh8RDO-v6tbaj

# **Project**

# 1. Imports
"""

import numpy as np
import pandas as pd
import nltk
import re
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
from sklearn.metrics import classification_report
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize

nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('punkt')
nltk.download('wordnet')

"""# 2. Dataset load"""

# Add the dataset the sample_data folder (on the left side)

# Load the dataset
train = pd.read_csv('/content/sample_data/train.txt', sep='\t', names=['title', 'from', 'genre', 'director', 'plot'])

x_train = train['genre']
y_train = train['plot']
#x_test = test['title', 'form', 'director', 'plot']


# Inspect the first few rows
print(train.head())

"""# 2. Pre-processing"""

# Pre-processing elements

stop = stopwords.words('english')
including = ['no', 'nor', 'not', 'but', 'against', 'only']
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# def processSentence(s):
#     words=re.split("\\s+",s)
#     stemmed_words=[porter_stemmer.stem(word=w) for w in words]
#     return ' '.join(stemmed_words)

def apply_preprocessing(text):

  lowered = text.lower()
  lowered_re = re.sub(r'\(.*?\)|[^a-zA-Z\s]', '', lowered)
  tokens = word_tokenize(lowered_re, "english")

  # Remove ponctuation
  for token in tokens:
        if(all(char in string.punctuation for char in token)):
            tokens.remove(token)

  filtered_tokens = [word for word in tokens if word not in stop]

  lemmatizer = WordNetLemmatizer()
  processed_tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in filtered_tokens]

  return ' '.join(processed_tokens)


# Apply preprocessing
y_train = y_train.apply(apply_preprocessing)


print(y_train[0])